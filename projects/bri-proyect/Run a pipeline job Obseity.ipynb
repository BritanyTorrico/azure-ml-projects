{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run scripts as a pipeline job\n",
        "\n",
        "A pipeline allows you to group multiple steps into one workflow. You can build a pipeline with components. Each component reflects a Python script to run. A component is defined in a YAML file which specifies the script and how to run it. \n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the  **azureml-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1717712214562
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: azure-ai-ml\r\n",
            "Version: 1.16.1\r\n",
            "Summary: Microsoft Azure Machine Learning Client Library for Python\r\n",
            "Home-page: https://github.com/Azure/azure-sdk-for-python\r\n",
            "Author: Microsoft Corporation\r\n",
            "Author-email: azuresdkengsysadmins@microsoft.com\r\n",
            "License: MIT License\r\n",
            "Location: /anaconda/envs/azureml_py38/lib/python3.8/site-packages\r\n",
            "Requires: azure-storage-blob, azure-core, isodate, tqdm, colorama, msrest, azure-storage-file-datalake, opencensus-ext-logging, pydash, strictyaml, azure-storage-file-share, azure-mgmt-core, pyyaml, pyjwt, typing-extensions, opencensus-ext-azure, azure-common, marshmallow, jsonschema\r\n",
            "Required-by: \r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1717712216593
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1717712216816
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the scripts\n",
        "\n",
        "You'll build a pipeline with two steps:\n",
        "\n",
        "1. **Prepare the data**: Fix missing data and normalize the data.\n",
        "1. **Train the model**: Trains a decision tree classification model.\n",
        "\n",
        "Run the following cells to create the **src** folder and the two scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1717712217045
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src folder created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Preparar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1717709978526
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/prep-data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/prep-data.py\n",
        "# importar librerías\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def main(args):\n",
        "    # leer datos\n",
        "    df = get_data(args.input_data)\n",
        "\n",
        "    cleaned_data = clean_data(df)\n",
        "\n",
        "    prepared_data = preprocess_data(cleaned_data)\n",
        "    \n",
        "    output_df = prepared_data.to_csv((Path(args.output_data) / \"obesity.csv\"), index=False)\n",
        "\n",
        "# función que lee los datos\n",
        "def get_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Contar las filas e imprimir el resultado\n",
        "    row_count = len(df)\n",
        "    print('Preparando {} filas de datos'.format(row_count))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# función que elimina valores nulos\n",
        "def clean_data(df):\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "# función que preprocesa los datos\n",
        "def preprocess_data(df):\n",
        "    # Definir las columnas numéricas y categóricas\n",
        "    numeric_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
        "    categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
        "\n",
        "    # Definir los pasos del pipeline para las características numéricas\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', MinMaxScaler())])\n",
        "\n",
        "    # Definir los pasos del pipeline para las características categóricas\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    # Combinar los pasos del pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Aplicar el preprocesamiento al DataFrame\n",
        "    X_prepared = preprocessor.fit_transform(df)\n",
        "    \n",
        "    # Obtener los nombres de las características transformadas para las columnas categóricas\n",
        "    cat_encoder = preprocessor.named_transformers_['cat']['onehot']\n",
        "    \n",
        "    # Construir los nombres de las características categóricas transformadas manualmente\n",
        "    cat_feature_names = []\n",
        "    for cat_feature in cat_encoder.categories_:\n",
        "        cat_feature_names.extend([f\"{cat_feature}_{value}\" for value in cat_feature])\n",
        "    \n",
        "    # Combinar los nombres de las características numéricas y categóricas\n",
        "    all_feature_names = numeric_features + cat_feature_names\n",
        "\n",
        "    # Convertir los datos transformados a un DataFrame con los nombres de las columnas adecuadas\n",
        "    X_prepared_df = pd.DataFrame(X_prepared, columns=all_feature_names)\n",
        "    \n",
        "    # Añadir la columna de destino 'NObeyesdad' al DataFrame preprocesado\n",
        "    X_prepared_df['NObeyesdad'] = df['NObeyesdad'].values\n",
        "    \n",
        "    return X_prepared_df\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--input_data\", dest='input_data', type=str)\n",
        "    parser.add_argument(\"--output_data\", dest='output_data', type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# ejecutar script\n",
        "if __name__ == \"__main__\":\n",
        "    # añadir espacio en los registros\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # analizar argumentos\n",
        "    args = parse_args()\n",
        "\n",
        "    # ejecutar función principal\n",
        "    main(args)\n",
        "\n",
        "    # añadir espacio en los registros\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/train-model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model.py\n",
        "# importar librerías\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    # leer datos\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # dividir datos\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        " #entrenar modelo\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # evaluar modelo\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "\n",
        "# función que lee los datos\n",
        "def get_data(data_path):\n",
        "    all_files = glob.glob(data_path + \"/*.csv\")\n",
        "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "    return df\n",
        "\n",
        "# función que divide los datos\n",
        "def split_data(df):\n",
        "    print(\"Dividiendo datos...\")\n",
        "    X = df.drop(columns=[\"NObeyesdad\"])\n",
        "    y = df[\"NObeyesdad\"]\n",
        "    \n",
        "    # codificar variables categóricas\n",
        "    X = pd.get_dummies(X)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# función que entrena el modelo\n",
        "def train_model(X_train, y_train, ):\n",
        "    print(\"Entrenando modelo...\")\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    \n",
        "    # Guardar el modelo en el path especificado\n",
        "    mlflow.sklearn.save_model(model, args.model_output)\n",
        "  \n",
        "\n",
        "    return model\n",
        "\n",
        "# función que evalúa el modelo\n",
        "def eval_model(model, X_test, y_test):\n",
        "    print(\"Evaluando modelo...\")\n",
        "    # calcular precisión\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print('Precisión:', acc)\n",
        "   \n",
        "    \n",
        "    # calcular AUC-ROC\n",
        "    y_probs = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test, y_probs, multi_class='ovr')  # Para multiclase\n",
        "    print('AUC-ROC:', auc)\n",
        "    \n",
        "\n",
        "    # Calcular F1-score\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')  # O 'micro' si prefieres\n",
        "    print('F1-score:', f1)\n",
        "    \n",
        "\n",
        "    # Calcular precisión y recall\n",
        "    precision = precision_score(y_test, y_pred, average='macro')  # O 'micro' si prefieres\n",
        "    recall = recall_score(y_test, y_pred, average='macro')  # O 'micro' si prefieres\n",
        "    print('Precisión:', precision)\n",
        "    print('Recall:', recall)\n",
        "  \n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--model_output\", dest='model_output',\n",
        "                        type=str)\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# ejecutar el script\n",
        "if __name__ == \"__main__\":\n",
        "    # añadir espacio en los registros\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # analizar argumentos\n",
        "    args = parse_args()\n",
        "\n",
        "    # ejecutar función principal\n",
        "    main(args)\n",
        "\n",
        "    # añadir espacio en los registros\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the components\n",
        "\n",
        "To define the component you need to specify:\n",
        "\n",
        "- **Metadata**: *name*, *display name*, *version*, *description*, *type* etc. The metadata helps to describe and manage the component.\n",
        "- **Interface**: *inputs* and *outputs*. For example, a model training component will take training data and the regularization rate as input, and generate a trained model file as output. \n",
        "- **Command, code & environment**: the *command*, *code* and *environment* to run the component. Command is the shell command to execute the component. Code usually refers to a source code directory. Environment could be an AzureML environment (curated or custom created), docker image or conda environment.\n",
        "\n",
        "Run the following cells to create a YAML for each component you want to run as a pipeline step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### prep-data.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting prep-data.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile prep-data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: prep_data\n",
        "display_name: Prepare training data\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  output_data:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python prep-data.py \n",
        "  --input_data ${{inputs.input_data}}\n",
        "  --output_data ${{outputs.output_data}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### train-model.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train-model.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile train-model.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_model\n",
        "display_name: Train a decision tree classifier model\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data: \n",
        "    type: uri_folder\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: mlflow_model\n",
        "code: ./src\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python train-model.py \n",
        "  --training_data ${{inputs.training_data}} \n",
        "  --model_output ${{outputs.model_output}} "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the components\n",
        "\n",
        "Now that you have defined each component, you can load the components by referring to the YAML files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1717712219099
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
        "train_decision_tree = load_component(source=parent_dir + \"./train-model.yml\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the pipeline\n",
        "\n",
        "After creating and loading the components, you can build the pipeline. You'll compose the two components into a pipeline. First, you'll want the `prep_data` component to run. The output of the first component should be the input of the second component `train_decision_tree`, which will train the model.\n",
        "\n",
        "The `diabetes_classification` function represents the complete pipeline. The function expects one input variable: `pipeline_job_input`. A data asset was created during setup. You'll use the registered data asset as the pipeline input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1717712220253
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def obesity_classification(pipeline_job_input):\n",
        "    clean_data = prep_data(input_data=pipeline_job_input)\n",
        "    train_model = train_decision_tree(training_data=clean_data.outputs.output_data)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
        "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
        "    }\n",
        "\n",
        "pipeline_job = obesity_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml:obesity-data:1\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can retrieve the configuration of the pipeline job by printing the `pipeline_job` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1717712220895
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display_name: obesity_classification\n",
            "type: pipeline\n",
            "inputs:\n",
            "  pipeline_job_input:\n",
            "    type: uri_file\n",
            "    path: azureml:obesity-data:1\n",
            "outputs:\n",
            "  pipeline_job_transformed_data:\n",
            "    type: uri_folder\n",
            "  pipeline_job_trained_model:\n",
            "    type: mlflow_model\n",
            "jobs:\n",
            "  clean_data:\n",
            "    type: command\n",
            "    inputs:\n",
            "      input_data:\n",
            "        path: ${{parent.inputs.pipeline_job_input}}\n",
            "    outputs:\n",
            "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: prep_data\n",
            "      version: '1'\n",
            "      display_name: Prepare training data\n",
            "      type: command\n",
            "      inputs:\n",
            "        input_data:\n",
            "          type: uri_file\n",
            "      outputs:\n",
            "        output_data:\n",
            "          type: uri_folder\n",
            "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
            "        ${{outputs.output_data}}\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2/src\n",
            "      is_deterministic: true\n",
            "  train_model:\n",
            "    type: command\n",
            "    inputs:\n",
            "      training_data:\n",
            "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
            "    outputs:\n",
            "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: train_model\n",
            "      version: '1'\n",
            "      display_name: Train a decision tree classifier model\n",
            "      type: command\n",
            "      inputs:\n",
            "        training_data:\n",
            "          type: uri_folder\n",
            "      outputs:\n",
            "        model_output:\n",
            "          type: mlflow_model\n",
            "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --model_output\n",
            "        ${{outputs.model_output}} '\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2/src\n",
            "      is_deterministic: true\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can change any parameter of the pipeline job configuration by referring to the parameter and specifying the new value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1717712221134
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display_name: obesity_classification\n",
            "type: pipeline\n",
            "inputs:\n",
            "  pipeline_job_input:\n",
            "    type: uri_file\n",
            "    path: azureml:obesity-data:1\n",
            "outputs:\n",
            "  pipeline_job_transformed_data:\n",
            "    mode: upload\n",
            "    type: uri_folder\n",
            "  pipeline_job_trained_model:\n",
            "    mode: upload\n",
            "    type: mlflow_model\n",
            "jobs:\n",
            "  clean_data:\n",
            "    type: command\n",
            "    inputs:\n",
            "      input_data:\n",
            "        path: ${{parent.inputs.pipeline_job_input}}\n",
            "    outputs:\n",
            "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: prep_data\n",
            "      version: '1'\n",
            "      display_name: Prepare training data\n",
            "      type: command\n",
            "      inputs:\n",
            "        input_data:\n",
            "          type: uri_file\n",
            "      outputs:\n",
            "        output_data:\n",
            "          type: uri_folder\n",
            "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
            "        ${{outputs.output_data}}\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2/src\n",
            "      is_deterministic: true\n",
            "  train_model:\n",
            "    type: command\n",
            "    inputs:\n",
            "      training_data:\n",
            "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
            "    outputs:\n",
            "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: train_model\n",
            "      version: '1'\n",
            "      display_name: Train a decision tree classifier model\n",
            "      type: command\n",
            "      inputs:\n",
            "        training_data:\n",
            "          type: uri_folder\n",
            "      outputs:\n",
            "        model_output:\n",
            "          type: mlflow_model\n",
            "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --model_output\n",
            "        ${{outputs.model_output}} '\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2/src\n",
            "      is_deterministic: true\n",
            "settings:\n",
            "  default_datastore: azureml:workspaceblobstore\n",
            "  default_compute: azureml:aml-cluster1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# change the output mode\n",
        "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
        "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = \"aml-cluster\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
        "\n",
        "# print the pipeline job again to review the changes\n",
        "print(pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit the pipeline job\n",
        "\n",
        "Finally, when you've built the pipeline and configured the pipeline job to run as required, you can submit the pipeline job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1717712228993
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading src (0.01 MBs): 100%|██████████| 9304/9304 [00:00<00:00, 201511.99it/s]\n",
            "\u001b[39m\n",
            "\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.MLFlowModelJobOutput'> and will be ignored\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_obesity</td><td>witty_bulb_16b39s08zv</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/witty_bulb_16b39s08zv?wsid=/subscriptions/08f17054-76c9-49db-b287-b10383c293e3/resourcegroups/rg-dp100-labs/workspaces/mlw-dp100-labs&amp;tid=1b604f7d-1cee-45b6-8431-4090d7becc32\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f955c7dd580>}, 'outputs': {'pipeline_job_transformed_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f955c7dd0a0>, 'pipeline_job_trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f955c7dd160>}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f955c7d5be0>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'obesity_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_transformed_data': {}, 'pipeline_job_trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'name': 'clean_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f955c7d5c70>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.pipeline_job_transformed_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f955c7d5fd0>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f955c7d5b20>}, 'component': 'azureml_anonymous:98656747-0c0e-4d34-9dc5-8e77667c4f9b', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '81c03cc1-3c57-4320-8202-72f5acfe35ca', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_model': Command({'parameters': {}, 'init': False, 'name': 'train_model', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f955c7d5fa0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_trained_model}}'}, 'inputs': {'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f955c7d5a00>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f955c7d5f70>}, 'component': 'azureml_anonymous:da954b2f-00e3-425f-b505-93fab95c1260', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'a2182668-a2dc-4ef4-a4e2-17033793264f', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'YAML.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'witty_bulb_16b39s08zv', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/08f17054-76c9-49db-b287-b10383c293e3/resourceGroups/rg-dp100-labs/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-labs/jobs/witty_bulb_16b39s08zv', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci123123/code/Users/torricobritany2', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f955c7dd4c0>, 'serialize': <msrest.serialization.Serializer object at 0x7f955c7dd490>, 'display_name': 'obesity_classification', 'experiment_name': 'pipeline_obesity', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/08f17054-76c9-49db-b287-b10383c293e3/resourceGroups/rg-dp100-labs/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-labs?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/witty_bulb_16b39s08zv?wsid=/subscriptions/08f17054-76c9-49db-b287-b10383c293e3/resourcegroups/rg-dp100-labs/workspaces/mlw-dp100-labs&tid=1b604f7d-1cee-45b6-8431-4090d7becc32', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"pipeline_obesity\"\n",
        ")\n",
        "pipeline_job"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
